
\documentclass[oneside]{report}

\title{Linear Algebra Study Guide}
\author{Orgho A. Neogi}
\date{20 September 2017}

\usepackage{amssymb}
\usepackage{amsmath}
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother

\usepackage{graphicx}

\usepackage{float}

\usepackage[colorlinks=true]{hyperref}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}[section]

\begin{document}

\maketitle

\chapter{Linear Equations in Linear Algebra}


\section{Systems of Linear Equations}

A system of linear equations has

\begin{enumerate}
  \item no solution, or
  \item exactly one solution, or
  \item infinitely many solutions.
\end{enumerate}

A system of linear equations is said to be consistent if it has either one solution or
infinitely many solutions; a system is inconsistent if it has no solution.

Given an example System of Equations:

\begin{alignat*}{8}
  &x_1 - &2x_2 + &x3  &= &0 \\
  &     &2x_2 - &8x_3 &= &8 \\
  &5x_1 &     - &5x_3 &= &10
\end{alignat*}

The matrix of coefficients is:

\begin{center}
$\begin{bmatrix}
  &1 &2 &1\\
  &0 &2 &-8\\
  &5 &0 &-5
\end{bmatrix}$
\end{center}

The augmented matrix is:
\begin{center}
  $\begin{bmatrix}[ccc|c]
    1 &2 &1  &0\\
    0 &2 &-8 &8\\
    5 &0 &-5 &10
  \end{bmatrix}$
\end{center}

A system of linear equations can be solved using elementary row operations.

Elementary row operations are:
\begin{enumerate}
  \item (Replacement) Replace one row by the sum of itself and a multiple of another row.
  \item (Interchange) Interchange two rows.
  \item (Scaling) Multiply all entries in a row by a nonzero constant.
\end{enumerate}

Row operations can be applied to any matrix and are reversible.


\section{Row Reduction and Echelon Forms}

\begin{definition}[Echelon Form]
A rectangular matrix is in echelon form (or row echelon form) if it has the
following three properties:

\begin{enumerate}
  \item All nonzero rows are above any rows of all zeros.
  \item Each leading entry of a row is in a column to the right of the leading entry of the row above it.
  \item All entries in a column below a leading entry are zeros.

  If a matrix in echelon form satisfies the following additional conditions, then it is in reduced row echelon form.

 \item The leading entry in each nonzero row is 1.
 \item Each leading 1 is the only nonzero entry in its column.
\end{enumerate}
\end{definition}

\begin{theorem}[Uniqueness of the Reduced Echelon Form]
  Each matrix is row equivalent to one and only one reduced echelon matrix.
\end{theorem}

\begin{definition}[Pivot Position]
  A pivot position in a matrix A is a location in A that corresponds to a leading 1
  in the reduced echelon form of A .
\end{definition}

\begin{definition}[Pivot Column]
  A pivot column is a column of A that contains
  a pivot position.
\end{definition}

\begin{theorem}[Existence and Uniqueness Theorem]
  A linear system is consistent if and only if the rightmost column of the augmented
  matrix is not a pivot columnâ€”that is, if and only if an echelon form of the
  augmented matrix has no row of the form:

  \begin{center}
    $\begin{bmatrix}
      &0 &\dots &0  &b
    \end{bmatrix}$
    With $b$ non-zero
  \end{center}
If a linear system is consistent, then the solution set contains either (i) a unique
  solution, when there are no free variables, or (ii) infinitely many solutions, when
  there is at least one free variable.
\end{theorem}

Using Row Reduction to Solve a Linear System:
\begin{enumerate}
  \item Write the augmented matrix of the system.
  \item Use the row reduction algorithm to obtain an equivalent augmented matrix in echelon form. Decide whether the system is consistent. If there is no solution, stop; otherwise, go to the next step.
  \item Write the system of equations corresponding to the matrix obtained in step 3.
  \item Rewrite each nonzero equation from step 4 so that its one basic variable is expressed in terms of any free variables appearing in the equation.
\end{enumerate}

\section{Vector Equations}

A matrix with only one column is called a column vector, or simply a vector.

Algebraic Properties of $\mathbb{R}^n$:

For all $\vec{u}$, $\vec{v}$, $\vec{w}$ in $\mathbb{R}^n$ and all scalars $c$ and $d$:
\begin{enumerate}
  \item $\vec{u} + \vec{v} = \vec{v} + \vec{u}$
  \item $(\vec{u} + \vec{v}) + \vec{w} = \vec{u} + (\vec{v} + \vec{w})$
  \item $\vec{u} + 0 = 0 + \vec{u} = 0$
  \item $\vec{u} + (-\vec{u}) = -\vec{u} + \vec{u} = 0$, where $-\vec{u}$ denotes $-1\vec{u}$
  \item $c(\vec{u} + \vec{v}) = c\vec{u} + c\vec{v}$
  \item $(c + d)\vec{u} = c\vec{u} + d\vec{u}$
  \item $c(d\vec{u}) = (cd)\vec{u}$
  \item $1\vec{u} = \vec{u}$
\end{enumerate}

A vector equation

\begin{center}
  $x_1\vec{a_1} + x_2\vec{a_2} + \dots + x_n\vec{a_n} = \vec{b}$
\end{center}

has the same solution set as the linear system whose augmented matrix is

\begin{center}
   $\begin{bmatrix}[cccc|c]
      \vec{a_1} &\vec{a_2} &\dots &\vec{a_n} &\vec{b}
    \end{bmatrix}$
\end{center}

\begin{definition}[span]
  If $\vec{v_1},\dots,\vec{v_p}$ are in $\mathbb{R}^n$, then the set of all linear combinations of $\vec{v_1},\dots,\vec{v_p}$ is denoted by $Span\{\vec{v_1},\dots,\vec{v_p}\}$ and is called the subset of $\mathbb{R}^n$ spanned by $\vec{v_1},\dots,\vec{v_p}$. That is $Span\{\vec{v_1},\dots,\vec{v_p}\}$ is the collection of all vectors that can be written in the form $c_1\vec{v_1} + c_2\vec{v_2} + \dots + c_p\vec{v_p}$ with $c_1,\dots,c_p$ scalars.
\end{definition}

\section{The Matrix Equation $A\vec{x} = \vec{b}$}

\begin{theorem}
If $A$ is an $m \times n$ matrix, with colums $\vec{a_1},\dots,\vec{a_n}$, and if $\vec{b}$ is in $\mathbb{R}^m$, the matrix equation
\begin{center}
  $A\vec{x} = \vec{b}$
\end{center}
has the same solution set as the vector equation
\begin{center}
  $x_1\vec{a_1} + x_2\vec{a_2} + \dots + x_n\vec{a_n} = \vec{b}$
\end{center}
which, in turn, has the same solution set as the system of linear equations whose
augmented matrix is
\begin{center}
   $\begin{bmatrix}[cccc|c]
      \vec{a_1} &\vec{a_2} &\dots &\vec{a_n} &\vec{b}
    \end{bmatrix}$
\end{center}
\end{theorem}

The equation $A\vec{x} = \vec{b}$ has a solution if and only if $\vec{b}$ is a linear combination of the columns of A .

\begin{theorem}
Let $A$ be an $m \times n$ matrix.Then the following statements are logically equivalent. That is, for a particular $A$ , either they are all true statements or they are all false.
\begin{enumerate}
  \item For each $\vec{b}$ in $\mathbb{R}^m$, the equation $A\vec{x} = \vec{b}$ has a solution
  \item Each $\vec{b}$ in $\mathbb{R}^m$ is a linear combination of the columns of $A$
  \item The columns of $A$ span $\mathbb{R}^m$
  \item $A$ has a pivot position in every row
\end{enumerate}
\end{theorem}

\begin{theorem}
If $A$ is an $m \times n$ matrix, $\vec{u}$ and $\vec{v}$ are vectors in $\mathbb{R}^n$, and $c$ is a scalar, then:
\begin{enumerate}
  \item $A(\vec{u}+\vec{v}) = A\vec{u} + A\vec{v}$
  \item $A(c\vec{u}) = c(A\vec{u})$
\end{enumerate}
\end{theorem}

\section{Solution Sets of Linear Systems}

The homogeneous equation $A\vec{x} = \vec{0}$ has a non trivial solution if and only if the equation has at least one free variable.

\begin{theorem}
  Suppose the equation $A\vec{x} = \vec{b}$ is consistent for some given $\vec{b}$, and let $\vec{p}$ be a solution. Then the solution set of $A\vec{x} = \vec{b}$ is the set of all vectors of the form $\vec{w} = \vec{p} + \vec{v_h}$, where $\vec{v_h}$ is any solution to the homogeneous equation $A\vec{x} = \vec{0}$.
\end{theorem}

Writing a solution set (of a consistent system) in parametric vector form:
\begin{enumerate}
  \item Row reduce the augmented matrix to reduced echelon form.
  \item Express each basic variable in terms of any free variables appearing in an
equation.
  \item Write a typical solution $\vec{x}$ as a vector whose entries depend on the free
variables, if any.
  \item Decompose $\vec{x}$ into a linear combination of vectors (with numeric entries) using
the free variables as parameters.
\end{enumerate}

\section{Linear Independence}

\begin{definition}[Linearly Independent]
  An indexed set of vectors $\{\vec{v_1}, \dots , \vec{v_p}\}$ in $\mathbb{R}^n$ is said to be linearly independent if the vector equation
  \begin{center}
    $x_1\vec{v_1}+x_2\vec{v_2}+\dots+x_p\vec{v_p} = 0$
  \end{center}
  has only the trivial solution. The set $\{\vec{v_1}, \dots , \vec{v_p}\}$ is said to be linearly dependent if it is not linearly independent.
\end{definition}

The columns of a matrix A are linearly independent if and only if the equation
$A\vec{x} = \vec{0}$ has only the trivial solution. \newline

A set of two vectors $\{\vec{v_1},\vec{v_2}\}$ is linearly dependent if at least one of the vectors is a multiple of the other. The set is linearly independent if and only if neither of the
vectors is a multiple of the other.

\begin{theorem}[Characterization of Linearly Dependent Sets]
  An indexed set $S = \{\vec{v_1}, \dots , \vec{v_p}\}$ of two or more vectors is linearly dependent if and only if at least one of the vectors in $S$ is a linear combination of the others. In
  fact, if $S$ is linearly dependent and $v_1 \neq 0$, then some $v_j$ (with $j > 1$ ) is a linear
  combination of the preceding vectors, $v_1, \dots, v_{j-1}$.
\end{theorem}

\begin{theorem}
  If a set contains more vectors than there are entries in each vector, then the set
is linearly dependent. That is, any set $S = \{\vec{v_1}, \dots , \vec{v_p}\}$ in $\mathbb{R}^n$ is linearly dependent if $p > n$.
\end{theorem}

\begin{theorem}
  If a set $S = \{\vec{v_1}, \dots , \vec{v_p}\}$ in $\mathbb{R}^n$ , contains the zero vector, then the set is linearly dependent.
\end{theorem}

\section{Introduction to Linear Transformations}

\begin{definition}[Linear Transformation]
  A transformation of mapping $T$ is linear if:
  \begin{enumerate}
    \item $T(\vec{u} + \vec{v}) = T(\vec{u}) + T(\vec{v})$ for all $\vec{u}, \vec{v}$ in the domain of $T$;
    \item $T(c\vec{u}) = cT(\vec{u})$ for all scalars $c$ and all $\vec{u}$ in the domain of $T$;
  \end{enumerate}
\end{definition}

Every matrix transformation is a linear transformation.

All Linear transformations can be written as matrix multiplication.

\section{The Matrix of a Linear Transformation}

\begin{theorem}
  Let $T:\mathbb{R}^n \rightarrow \mathbb{R}^m$ be a linear transformation. Then there exists a unique matrix
$A$ such that:
\begin{center}
  $T(\vec{x})=A\vec{x}$ For all $\vec{x}$ in $\mathbb{R}^n$
\end{center}
\end{theorem}

\chapter{Matrix Algebra}


\end{document}
